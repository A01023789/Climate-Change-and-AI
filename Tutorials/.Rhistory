pr.out.Boston$center
pr.out.Boston$scale
pr.out$rotation
pr.out.Boston$sdev
print(pr.out.Boston)
pr.varBoston <- pr.out.Boston$sdev^2
pr.varBoston
pveBoston <- pr.varBoston/sum(pr.varBoston)
pveBoston
summary(pr.out.Boston)
plot(cumsum(pveBoston), xlab="Componente Principal", ylab="Proporción Acumulada de la Varianza Explicada", ylim=c(0,1), type='b')
text(cumsum(pveBoston),
labels = round(cumsum(pveBoston),3),
cex = 0.8, pos = 1, col = "black")
library(MASS)
attach(Boston)
pr.out.Boston <- prcomp(Boston, scale=TRUE)
names(pr.out.Boston)
#Verificar los componentes
pr.out.Boston$center
pr.out.Boston$scale
#crear componentes principales
pr.out.Boston$rotation
library(MASS)
attach(Boston)
pr.out.Boston <- prcomp(Boston, scale=TRUE)
names(pr.out.Boston)
#Verificar los componentes
pr.out.Boston$center
pr.out.Boston$scale
#crear componentes principales
pr.out.Boston$rotation
pr.out.Boston$sdev
print(pr.out.Boston)
pr.varBoston <- pr.out.Boston$sdev^2
pr.varBoston
pveBoston <- pr.varBoston/sum(pr.varBoston)
pveBoston
summary(pr.out.Boston)
plot(cumsum(pveBoston), xlab="Componente Principal", ylab="Proporción Acumulada de la Varianza Explicada", ylim=c(0,1), type='b')
text(cumsum(pveBoston),
labels = round(cumsum(pveBoston),3),
cex = 0.8, pos = 1, col = "black")
knitr::opts_chunk$set(echo = TRUE)
#Guardar en states los nombres de la fila del conjunto de datos
states <- row.names(USArrests )
states
#Variabes del USArrests
names(USArrests )
#2 para que nos saque el promedio por columnas
apply(USArrests, 2, mean)
#Función para obtener varianzas
apply(USArrests, 2, var)
#Componentes principales con prcomp, le adjuntamos la base de datos, scala =TRUE para homologar la varianza
pr.out <- prcomp(USArrests, scale=TRUE)
names(pr.out)
#Verificamos los datos modificados gracias a =TRUE
pr.out$center
pr.out$scale
pr.out$rotation
#Componente principal uno, se crea un índice de una característica que contiene los cuatro tipos de variables contenidos en una variable
#Explica un pedazo de la varianza, en este caso son cuatro
#x nos da la carga que tiene cada uno de los componentes principales a cada uno de los estados
pr.out$x
dim(pr.out$x)
#utilizamos biplot para graficar los primeros dos componentes principales que son los que normalmente explican la mayor parte de los datos
biplot(pr.out, scale=0)
#Para no tener valores negativos multiplicamos todo por -1
pr.out$rotation <- -1*(pr.out$rotation)
pr.out$x<- -1*(pr.out$x)
biplot(pr.out, scale=0)
#Saber desviación estándar
pr.out$sdev
print(pr.out)
#Obtener la varianza a partir de la desviación estándar
pr.var <- pr.out$sdev^2
pr.var
#Guardamos el porcentaje de lo que explica cada uno de los componentes
pve <- pr.var/sum(pr.var)
pve
summary(pr.out)
#Ploteo para ver la proporción de la variables explicada
plot(pve, xlab="Componente Principal", ylab="Proporción de la Varianza Explicada", ylim=c(0,1), type='b')
text(pve,
labels = round(pve,3),
cex = 0.8, pos = 1, col = "black")
#Para ver como va llegando al 100%
plot(cumsum(pve), xlab="Componente Principal", ylab="Proporción Acumulada de la Varianza Explicada", ylim=c(0,1), type='b')
text(cumsum(pve),
labels = round(cumsum(pve),3),
cex = 0.8, pos = 1, col = "black")
a <- c(1,2,8,-3)
#Cumsum suma los elementos
cumsum(a)
library(MASS)
attach(Boston)
pr.out.Boston <- prcomp(Boston, scale=TRUE)
names(pr.out.Boston)
#Verificar los componentes
pr.out.Boston$center
pr.out.Boston$scale
#crear componentes principales
pr.out.Boston$rotation
pr.out.Boston$sdev
print(pr.out.Boston)
pr.varBoston <- pr.out.Boston$sdev^2
pr.varBoston
pveBoston <- pr.varBoston/sum(pr.varBoston)
pveBoston
summary(pr.out.Boston)
plot(cumsum(pveBoston), xlab="Componente Principal", ylab="Proporción Acumulada de la Varianza Explicada", ylim=c(0,1), type='b')
text(cumsum(pveBoston),
labels = round(cumsum(pveBoston),3),
cex = 0.8, pos = 1, col = "black")
#Según la gráfica obtenida, podemos apreciar que los primeros 5 componentes principales explican el 80.6% de la varianza.
knitr::opts_chunk$set(echo = TRUE)
#Los de clasificación nos ayudan a clasificarint
#Los de regresión a hacer predicciones
library(tree)
library(ISLR)
attach(Carseats)
#Metas menores o iguales a 8 dirá que no son ventas altas y los que no cumplan con esa categoría dirán Yes
High<-ifelse(Sales <=8,"No","Yes")
#Necesitamos unir la variable al data set utilizando data.frame
Carseats<-data.frame(Carseats,High)
#revisar el tipo de clase que tenemos en High
class(Carseats$High)
#Ya que es categórica se debe utilizar la función as.factor para que reescribir las características de las variables
#Tree necesita que las variables sean factores por lo que se necesita utilizar esa función
Carseats$High = as.factor(Carseats$High)
#Se revisa para comprobar que sí sea factor
class(Carseats$High)
#Función para clasificar el arbol de decisión con respecto a si existen ventas altas o no
#Utilizamos tree para hacerlo, eliminamos la variable Sales para no dobletear las ventas
#se debe específicar la base de datos ya que fue modificada
tree.carseats<-tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
#Para agragar los textos
text(tree.carseats, pretty=0, cex=0.8)
knitr::opts_chunk$set(echo = TRUE)
#Los de clasificación nos ayudan a clasificarint
#Los de regresión a hacer predicciones
library(tree)
library(ISLR)
attach(Carseats)
#Metas menores o iguales a 8 dirá que no son ventas altas y los que no cumplan con esa categoría dirán Yes
High<-ifelse(Sales <=8,"No","Yes")
#Necesitamos unir la variable al data set utilizando data.frame
Carseats<-data.frame(Carseats,High)
#revisar el tipo de clase que tenemos en High
class(Carseats$High)
#Ya que es categórica se debe utilizar la función as.factor para que reescribir las características de las variables
#Tree necesita que las variables sean factores por lo que se necesita utilizar esa función
Carseats$High = as.factor(Carseats$High)
#Se revisa para comprobar que sí sea factor
class(Carseats$High)
#Función para clasificar el arbol de decisión con respecto a si existen ventas altas o no
#Utilizamos tree para hacerlo, eliminamos la variable Sales para no dobletear las ventas
#se debe específicar la base de datos ya que fue modificada
tree.carseats<-tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
#Para agragar los textos
text(tree.carseats, pretty=0, cex=0.8)
#Al escribir el nombre del objeto nos dará cada una de las ramas que tiene el arbol y así observar los predictores de nuestra función
tree.carseats
#Estimamos el error con un conjunto de entrenamiento y uno de prueba
set.seed(123)
train<-sample(1:nrow(Carseats), 200)
#toma todos los datos que no sean variabled de train
Carseats.test<-Carseats[-train, ]
#De la vairable high toma todos los que nos son de entrenaiento
High.test<-High[-train]
#arbol con argumento extra donde especificamos que se debe de hacer con el conjunto de entrenamiento
tree.carseats<-tree(High~.-Sales,Carseats,subset=train)
#Predicción para probar el modelo en el conjunto de prueba, se agrega el type=class para que nos regrese una predicción de clase real
tree.pred<-predict(tree.carseats,Carseats.test,type="class")
#Matriz de confusión
table(tree.pred,High.test)
(87+65)/200
#Validación cruzada, usamos cv.tree para obtener la validación cruzada del modelo en el que utilizamos el arbol de decisión
set.seed(123)
#Utilizamos el conjunto de entrenamiento
#prune.misclass para que la tasa de error de la clasificación nos guíe en el proceso de validación obteniendo el arbol más pequeño posbible
cv.carseats<-cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
#dev tasa de error de la validación cruzada
cv.carseats
#ploteamos el resultado de carseats donde hicimos la validación cruzada
plot(cv.carseats$size ,cv.carseats$dev ,type="b")
prune.carseats<-prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred<-predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
#3
(85+58)/200
#8
(88+66)/200
#13
(86+65)/200
prune.carseats<-prune.misclass(tree.carseats, best=13)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred<-predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(86+65)/200
library(MASS)
set.seed(123)
#Para la función trrain estamos utilizando la mitad del conjuntos de datos
train <- sample(1:nrow(Boston), nrow(Boston)/2)
Boston.data.test<-Boston[-train, ]
library(MASS)
set.seed(123)
#Para la función trrain estamos utilizando la mitad del conjuntos de datos
train <- sample(1:nrow(Boston), nrow(Boston)/2)
Boston.data.test<-Boston[-train, ]
#Se tiene que revisar si funciona el signo de exclamación o menos
tree.boston<-tree(medv~., data=Boston, subset=train)
summary(tree.boston)
#Plot del arbol de regresión
plot(tree.boston)
text(tree.boston, pretty=0)
#Validación para ver si se puede hacer un mejor arbol
cv.boston<-cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type="b")
#Podar con la función prune.tree
prune.boston<-prune.tree(tree.boston, best=4)
plot(prune.boston)
text(prune.boston, pretty=0)
tree.boston
#Predicción para obtener el valor medio de las casas
boston.pred<-predict(tree.boston, newdata=Boston.data.test)
boston.test<-Boston[-train,"medv"]
sqrt(mean((boston.pred-boston.test)^2))
library(randomForest)
set.seed(123)
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
library(randomForest)
#Bagging es un tipo de random forest
library(randomForest)
set.seed(123)
#mtry es para que considere los 13 predictores sean parte del árbol, importance true para que de la importancia de cada una de las variables dentro del modelo
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
#Realizar la comparación con el conjunto de prueba
boston.pred.bag <- predict(bag.boston, newdata=Boston.data.test)
#Diferencia
plot(boston.pred.bag, boston.test)
abline(0,1)
#Calcular el valor medio al cuadrado
mean((boston.pred.bag-boston.test)^2)
#Correr con menos para ver si se reduce el error
#ntree para indicar el número de árboles que quiero
set.seed(123)
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
boston.pred.bag <- predict(bag.boston, newdata=Boston.data.test)
mean((boston.pred.bag-boston.test)^2)
#Disminuir el número de predictores a 6
set.seed(123)
rf.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=6,importance =TRUE)
boston.pred.rf <- predict(rf.boston ,newdata=Boston.data.test)
mean((boston.pred.rf-boston.test)^2)
#Conocer la importancia de los predictores que se están utilizando
importance(rf.boston)
varImpPlot(rf.boston)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
library(gbm)
#Hace una interacción mucho más grande de la información para obtener resultados más confiables
set.seed(123)
boost.boston<-gbm(medv~.,data=Boston[train,],
distribution="gaussian",n.trees=5000,interaction.depth=4)
library(gbm)
#Hace una interacción mucho más grande de la información para obtener resultados más confiables
set.seed(123)
#Aquí el conjunto de datos se pone directamente
boost.boston<-gbm(medv~.,data=Boston[train,],
distribution="gaussian",n.trees=5000,interaction.depth=4)
#Para árbol de regresión se utiliza gaussian
#Para árbol de clasificación se utiliza "bernoulli"
summary(boost.boston)
par(mfrow=c(1,2))
#Plot solo de las variables más importantes
plot(boost.boston, i="rm")
plot(boost.boston, i="lstat")
library(tree)
library(ISLR)
attach(OJ)
summary(OJ)
library(tree)
library(ISLR)
attach(OJ)
summary(OJ)
set.seed(555)
trainT<-sample(1:nrow(OJ),800)
OJ.test<-OJ[-train,]
tree.OJT<-tree(Purchase~.,OJ,subset=train)
summary(tree.OJT)
tree.OJT<-tree(Purchase~.,OJ,subset=trainT)
summary(tree.OJT)
#Se puede apreciar que
plot(tree.OJT)
text(tree.OJT,pretty = 0)
?OJ
OJ.predT<- predict(tree.OJT,newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
OJ.predT<- predict(tree.OJT,newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
set.seed(555)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
set.seed(555)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
set.seed(555)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
library(tree)
library(ISLR)
attach(OJ)
summary(OJ)
set.seed(555)
trainT<-sample(1:nrow(OJ),800)
OJ.test<-OJ[-train,]
set.seed(555)
tree.OJT<-tree(Purchase~.,OJ,subset=trainT)
summary(tree.OJT)
#Se puede apreciar que la tasa de error de entrenamiento es de 13.75% y la cantidad de nodos terminales del árbol es de 9
plot(tree.OJT)
text(tree.OJT,pretty = 0)
#Gracias a la gráfica se puede apreciar que la variable más significativa para explicar si alguien eligió comprar Citrus Hill o Minute Maid es LoyalCH que indica la fidelidad de los clientes a la marca Citrus Hill
#Posteriormente, la variable más significativa es la diferencia de precios entre una marca y otra.
set.seed(555)
OJ.predT<- predict(tree.OJT, newdata = OJ.test,type="class")
table(OJ.predT,OJ.test$Purchase)
library(tree)
library(ISLR)
attach(OJ)
View(OJ)
summary(OJ)
set.seed(555)
train<-sample(1:nrow(OJ), 800)
OJ.test<-OJ[-train, ]
library(tree)
library(ISLR)
attach(OJ)
summary(OJ)
set.seed(555)
train<-sample(1:nrow(OJ), 800)
OJ.test<-OJ[-train, ]
tree.OJ<-tree(Purchase~.,OJ,subset=train)
summary(tree.OJ)
#Se puede apreciar que la tasa de error de entrenamiento es de 13.75% y la cantidad de nodos terminales del árbol es de 9
plot(tree.OJ)
text(tree.OJ, pretty=0)
#Gracias a la gráfica se puede apreciar que la variable más significativa para explicar si alguien eligió comprar Citrus Hill o Minute Maid es LoyalCH que indica la fidelidad de los clientes a la marca Citrus Hill
#Posteriormente, la variable más significativa es la diferencia de precios entre una marca y otra.
OJ.pred<-predict(tree.OJ,newdata=OJ.test, type = "class")
table(OJ.pred,OJ.test$Purchase)
OJ.pred<-predict(tree.OJ,newdata=OJ.test, type = "class")
table(OJ.pred,OJ.test$Purchase)
#Gracias a la matriz de confusión podemos sacar la tasa de error de la prueba
(28+31)/270
OJ.pred<-predict(tree.OJ,newdata=OJ.test, type = "class")
table(OJ.pred,OJ.test$Purchase)
#Gracias a la matriz de confusión podemos sacar la tasa de error de la prueba
(28+31)/270
#La tasa de error de la prueba es del 21.8%
cv.tree.OJ<-cv.tree(tree.OJ,FUN=prune.misclass)
plot(cv.tree.OJ$size,cv.tree.OJ$dev)
prune.OJ<-prune.tree(tree.OJ, best=5)
plot(prune.OJ)
text(prune.OJ, pretty=0)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tree)
library(ISLR)
attach(Carseats)
High<-ifelse(Sales <=8,"No","Yes")
Carseats<-data.frame(Carseats,High)
class(Carseats$High)
Carseats$High = as.factor(Carseats$High)
class(Carseats$High)
tree.carseats<-tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty=0, cex=0.5)
tree.carseats
set.seed(123)
train<-sample(1:nrow(Carseats), 200)
Carseats.test<-Carseats[-train, ]
High.test<-High[-train]
tree.carseats<-tree(High~.-Sales,Carseats,subset=train)
tree.pred<-predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(87+65)/200
set.seed(123)
cv.carseats<-cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
plot(cv.carseats$size ,cv.carseats$dev ,type="b")
prune.carseats<-prune.misclass(tree.carseats,best=8)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred<-predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
#3
(85+58)/200
#8
(88+66)/200
#13
(86+65)/200
prune.carseats<-prune.misclass(tree.carseats, best=13)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred<-predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(86+65)/200
prune.carseats<-prune.misclass(tree.carseats, best=13)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred<-predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(86+65)/200
library(MASS)
set.seed(123)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
Boston.data.test<-Boston[-train, ]
tree.boston<-tree(medv~., data=Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0)
cv.boston<-cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type="b")
prune.boston<-prune.tree(tree.boston, best=4)
plot(prune.boston)
text(prune.boston, pretty=0)
tree.boston
boston.pred<-predict(tree.boston, newdata=Boston.data.test)
boston.test<-Boston[-train,"medv"]
sqrt(mean((boston.pred-boston.test)^2))
library(randomForest)
set.seed(123)
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
boston.pred.bag <- predict(bag.boston, newdata=Boston.data.test)
plot(boston.pred.bag, boston.test)
abline(0,1)
mean((boston.pred.bag-boston.test)^2)
set.seed(123)
bag.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
boston.pred.bag <- predict(bag.boston, newdata=Boston.data.test)
mean((boston.pred.bag-boston.test)^2)
set.seed(123)
rf.boston<-randomForest(medv~.,data=Boston,subset=train,mtry=6,importance =TRUE)
boston.pred.rf <- predict(rf.boston ,newdata=Boston.data.test)
mean((boston.pred.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
set.seed(123)
boost.boston<-gbm(medv~.,data=Boston[train,],
distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
library(tree)
library(ISLR)
attach(OJ)
View(OJ)
summary(OJ)
set.seed(555)
train<-sample(1:nrow(OJ), 800)
OJ.test<-OJ[-train, ]
treeOJ<-tree(Purchase~.,OJ,subset=train)
summary(treeOJ)
#Tasa error de train = 0.137
#Nodos: 9
plot(treeOJ)
text(treeOJ, pretty=0)
OJpred<-predict(treeOJ,newdata=OJ.test, type = "class")
table(OJpred,OJ.test$Purchase)
(28+31)/270
#0.218 tasa error
cv.tree.oj <- cv.tree(treeOJ,FUN=prune.misclass)
plot(cv.tree.oj$size,cv.tree.oj$dev)
#El tamaño 5 es el árbol que tiene una menor tasa de error de clasificaión válida cruzada. A partir del tamaño 2 el error se minimiza, pero sigue manteniendose constante en el tamaño 5, aunque con la menor tasa de error entre todos los tamaós. Por todo esto, el tamaño óptimo es 5 como ha sido mencionado anteriormente, evitando la sobreramificación.
#Validación cruzada:
#Si conduce a un árbol podado (de 9 nodos inicialmente). El arbol nos indica que el más óptimo y el mejor es de 5 nodos.
prune.OJ<-prune.tree(treeOJ, best=5)
plot(prune.OJ)
text(prune.OJ, pretty=0)
library(ISLR)
attach(OJ)
View(OJ)
summary(OJ)
set.seed(555)
train<-sample(1:nrow(OJ), 800)
OJ.test<-OJ[-train, ]
library(tree)
library(ISLR)
attach(OJ)
summary(OJ)
set.seed(555)
train<-sample(1:nrow(OJ), 800)
OJ.test<-OJ[-train, ]
